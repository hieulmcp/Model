## SUMMARY DATA ANALYSIS - TỔNG QUAN VỀ PHÂN TÍCH DỮ LIỆU
data_analysis = '''
    A. Quy trình Data Analysis
        1. Định nghĩa
            - Data analysis - phân tích dữ liệu là quá trình kiểm tra, làm sach, 
            chuyển đổi và mô hình hóa dữ liệu với mục tiêu khám phá thông tin hữu ích, thông báo và hỗ trợ ra quyết định.
            - Sử dụng trong nhiều lĩnh vực kinh doanh, khoa học và khoa học xã hội
            - Trong kinh doanh, phân tích dữ liệu đóng vai trò giúp đưa ra quyết định khoa học hơn và giúp doanh nghiệp hoạt động hiệu quả hơn
        2. Mục tiêu chính của Data analysis - Phân tích dữ liệu
            - Để có được thông tin hành động liên quan đến công việc/Doanh nghiệp...
            - Đối với nhà quản trị thì làm việc trên miền dữ liệu, đễ hỗ trợ các quyết định và hành động kinh doanh
            - Cho nên việc hiểu các lý thuyết và kỹ thuật trong các lĩnh vực phân tích dữ liệu như thống kê, khai thác dữ liệu và phân tích dự đoán 
            (statistics, data mining và predictive analytics) cực kỳ quan trọng.
        3. Một số kỹ thuật phân tích dữ liệu
            Phần 1: Phân tích tương quan - Correlation Analysis
                - Correlation là một kỹ thuật thống kê có thể cho thấy mối quan hệ giữa 2 biến
                - Xác minh và làm rõ mối liên hệ giữa 2 thuộc tính với nhau có mối quan hệ, ràng buộc, chi phối.... đọc lập hay không độc lập
            Phần 2: Phân tích hồi quy - Regression Analysis
                - Đây là một trong những kỹ thuật phân tích dữ liệu thống kê điều tra mối quan hệ giữa các biến khác nhau
                - Được sử dụng khi nghi ngờ rằng một trong các biến có thể ảnh hưởng đến (các) biến khác
                - Phân tích hồi quy có thể được sử dụng khi cố gắng tạo dự báo hoặc phân tích mối quan hệ giữa các biến
            Phần 3: Trực quan hóa dữ liệu - Data visualization
                - Trực quan hóa dữ liệu giúp mọi người hiểu được thông tin quan trọng trong dữ liệu bằng cách hiển thị nó trong bối cảnh trực quan
                - Một trong những kỹ thuật phân tích quan trọng nhất hiện nay khi thế giới đầy dữ liệu
                - Đặc biệt hữu ích khi chúng ta tìm cách năm bắt những hiểu biết sâu sắc từ một khối lượng lớn dữ liệu một cách nhanh chóng
                - Có thể dùng các tool: power BI, Google studio, tablue...
            Phần 4: Phân tích tình huống/ Kịch bản - Scenario Analysis
                - Phân tích tình huống/ kịch bản là phân tích các sự kiện có thể xảy ra trong tương lai với kết quả thay thế
                - Được sử dụng khi chung ta có 1 số lựa chọn thay thế tiềm năng nhưng không chắc chắn về quyết định đưa ra
                - Phân tích kịch bản là một công cụ quan trọng trong nhiều DN và được sử dụng rộng rãi để đưa ra dự đoán cho tương lai
            Phần 5: Khai thác dữ liệu - Data Mining
                - Khai thác dữ liệu đôi khi là khám phá dữ liệu/ kiến thức là 1 quá trình phân tích dữ liệu được thiết kế được làm việc với khối 
                lượng dữ liệu lớn giúp phát triển các mẫu, mỗi quan hệ hoặc thông tin có liên quan có thể cải thiện hiệu suất
                - Ví dụ: Nhà bán lẽ có thể sử dụng khai thác dữ liệu để công ty thẻ tín dụng để xuất sản phẩm cho chủ thẻ dựa trên phân tích chi 
                tiêu hàng tháng của họ.
    4. Kỹ thuật nâng cao
            Phần 1: Mạng neuron - Neural Networks
                - Mạng neuron là kỹ thuật lấy cảm hứng từ cách mạng thần kinh sinh học, như não, để xử lý thông tin
                - Mục đích của các mạng này là mô phỏng quá trình học tập của bộ não con người trên máy tính để tạo điều kiện cho việc 
                ra quyết định trong trí tuệ nhân tạo (AI)
                - Kỹ thuật Neural Networks có thể được sử dụng để trích xuất các mẫu và 
                phát hiện các xu hướng quá phức tạp để được xác định bởi con người hoặc các kỹ thuật máy tính khác.
                - Mạng lưới thần kinh được đào tạo có thể xem như 1 chuyên gia có khả năng đưa ra các dự đoán với 
                các tình huống đã cho và trả lời câu hỏi "what if"
                - Khả năng tự học hỏi dựa trên kỹ thuật này
            Phần 2: A/B testing
                - A/B testing có thể gọi là thử nghiệm phân tách. 
                Đây là 1 phương pháp so sánh hai phiên bản của một trang web hoặc ứng dụng với nhau để xác định phiên bản nào hoạt động tốt hơn.
                - Kỹ thuật A/B testing thường được sử dụng trong tiếp thị kỹ thuật số để kiểm tra phản ứng của người dùng đồi 
                với 1 tin nhắn và xem cái nào hoạt động tốt nhất, kiểm tra giả thuyết trong việc ra mắt một hoạt động mới, 
                một chiến dịch quảng cảo hoặc 1 thông điệp quảng cáo
                - Phiên bản nào hoạt động tốt hơn thì làm biên bản chính để hoạt động của chúng ta
        5. Công cụ dành cho phân tích dữ liệu        
            Phần 1: Công cụ chính tạo nên phân tích dữ liệu là dòng và bảng
                - Tạo bảng phân phối tân suất (frequency distribution table) để hiện thị dữ liệu cột và dòng mỗi tương quan giữa 2 thuộc tính
                - Rất hiểu ít với 2 thuộc tính categorical
            Phần 2: Công chính tiếp theo đó là trực quan hóa dữ liệu
                - Dùng các tools để thực xự rất đẹp
        6. Ba quy tắc phân tích dữ liệu
            Quy tắc 1: Nhìn vào dữ liệu và suy nghĩ vào những gì chúng ta muốn biết ?
                - Đặt câu hỏi và đóng khung câu hỏi và xem các giả thuyết
                - Ví dụ: Chung ta có muốn chứng minh trái đất hình cầu ?
            Quy tắc 2: Ước tính 1 xu hướng trung tâm (central Tendency) cho dữ liệu
                - Ví dụ: mean, median
                - Cái nào chúng sử dụng sẽ phụ thuộc vào giả thiết của trong quy tắc 1
            Quy tắc 3: Xem xét các ngoại lệ cho xu hướng trung tâm
                - Nếu đã đo trung bình, hãy nhìn vào số liệu không phải trung bình.
                - Nếu đã đo được 1 trung vị, hãy nhìn vào những con số mà không đáp ứng được kỳ vọng đó
                - Ngoại lệ giúp bạn phát hiện vấn đề với kết luận
                - Nhưng nếu nhìn vào các ngoại lệ, bạn có thể thấy họ đang nhận được 100 trong 3 lớp và 40 trong ba lớp khác. 
                => Trong trường hợp này trung bình là hoàn toàn sai lệch.
        7. Vấn đề của phân tích dữ liệu
            - Tại sao nhiều trường hợp phân tích dữ liệu kết thúc với tuyên bố bị lỗi ? 
            Một trong những lý do chính là một quá trình phức tạp và tẻ nhạt. Nó không bao giờ dễ dàng như đưa số vào máy tính.
                * Tập dữ liệu không được xử lý sách sẽ và chuẩn hóa bên trong
                * Sử dụng sai phương pháp
                * Khả năng lỗi phát sinh trong quá trình làm việc rất là cao
                * Nhiệm vụ sử dụng công cụ nào cho hợp lý => Những biểu đồ quyết định phù hợp
            - Một số vấn đề có thể dẫn đến phân tích dữ liệu bị lỗi
                * Không có kỹ năng phân tích đúng
                * Sử dụng công cụ sai để phân tích dữ liệu. Ví dụ: sử dụng z-score khi dữ liệu không có phân phối chuẩn
                * Để bias ảnh hưởng đến kết quả => Bị chi phối bởi định kiến của bên ngoài và môi trường
                * Không tìm ra ý nghĩa thống kê => Không xác định được yêu cầu của bài toán và giả thiết không đúng nên đi lòng vòng
                * Phát biểu không chính xác null hypothesis và alternate hypothesis
                * Sử dụng graph và chart không chính xác gây hiểu lầm
        8. Quy trình Data analysis
            - Business understanding => Data Requirements => Data conllection => Data Pre-processing 
            => Exploratory Data => Modeling Algorithms => Data product => Communication
            - Quy trình chi tiết
                Step1: Business understanding
                    * Trước khi cố gắng rút ra thông tin chi tiết hữu ích từ dữ liệu, điều cần thiết là xác định vấn đề kinh doanh cần giải quyết, 
                    cố gắng hiểu rõ về những gì doanh nghiệp cần trích xuất từ dữ liệu	
                    * Xác định các vấn đề (problem denfinition) là động lực để thực hiện kế hoạch phân tích dữ liệu. Các nhiệm vụ là xác định mục tiêu 
                    của phân tích, xác định các công việc, vạch ra vai trò và trách nhiệm, thu thập trạng thái hiện tại của dữ liệu,
                    xác định thời gian biểu và thực hiện phân tích chi phí / lợi nhuận. Từ đó một kế hoạch thực thi có thể được tạo ra.
                Step2: Data Requirements
                    * Dữ liệu là cần thiết để làm đầu vào cho phân tích, được chỉ định dựa trên yêu cầu của những người chỉ đạo phân tích 
                    hoặc khách hàng (những người sẽ sử dụng sản phẩm của phân tích). Mẫu mà dữ liệu sẽ được thu thập được gọi là 
                    một đơn vị thủ nghiệm ( Ví dụ 1 người hay 1 quần thể)
                    * Các biến cụ thể liên quan đến người (ví dụ: tuổi và thu nhập) có thể được chỉ định và thu được. 
                    Dữ liệu có thể là numerical hoặc categorical
                Step3: Data Collection
                    * Dữ liệu được thu thập từ nhiều nguồn khác nhau
                    * Các yêu cầu có thể được các nhà phân tích truyền đạt tời người gián sát dữ liệu
                    * Dữ liệu cũng được thu thập từ các cảm biến trong môi trường, chẳng hạn như camera giao thông, vệ tinh, thiết bị..
                    * Nó cũng có thể lấy từ các cuộc phỏng vấn, tải xuống từ các nguồn trực tuyến hoặc tài liệu đọc
                    * Bám sát vào data requirement để thu thập dữ liệu cho đúng
                Step4: Data pre-processing
                    * Dữ liệu thu được ban đầu phải được xử lý hoặc tổ chức để phân tích
                    * Các thao tác trong Data pre-processing
                        1. Data cleaning: làm sạch dữ liệu loại bỏ những lỗi tiềm ẩn bên trong
                        2. Data normalization: Chuẩn hóa dữ liệu - scaler dữ liệu
                        3. Data transformation: Biến đổi dữ liệu, biến đổi các thuộc tính, các tính năng phù hợp hơn
                        4. Missing values imputation: xử lý những dữ liệu phù hợp thay thế sao hợp lý hoặc loại bỏ
                        5. Data integration: tích hợp dữ liệu, cách đọc từ nhiều file, nối từ nhiều file, tích hợp bộ thống nhất
                        6. Noise identification: xử lý những thuộc tính hoặc giá trị nhiểu nào không
                Step5: Explpratory Data Analysis => Rất quan trọng
                    * Một dữ liệu đã được làm sạch, nó có thể được phân tích
                    * Các nhà phân tích có thể áp dụng các kỹ thuật được gọi là phân tích dữ liệu thăm dò có thể dẫn đến việc làm sạch 
                    dữ liệu bổ sung hoặc yêu cầu bổ sung cho dữ liệu => Các hoạt động này có thể lặp đi lặp lại cho đến khi về với bản chất
                    * Thống kê mô tả, những trung bình và trung vị có thể được tạo để giúp hiểu dữ liệu
                    * Trực quan hóa dữ liệu cũng có thể được sử dụng để kiểm tra dữ liệu ở định dạng đồ họa,
                    để có được cái nhìn sâu sắc về các thông điệp trong dữ liệu
                    * Vừa phân tích và khám phá dữ liệu để khám phá xem có dữ liệu làm sạch bổ sung hoặc thêm dữ liệu bổ sung
                Step6: Modeling & Algorithms
                    * Các công thức hoặc các mô hình toán học được gọi là thuật toán có thể được áp dụng cho dữ liệu để xác định mối quan hệ giữa các biến,
                    chẳng hạn như tương quan hoặc quan hệ nhân quả
                    * Các mô hình có thể được phát triển đánh giá một biến cụ thể trong dữ liệu dựa trên các biến khác trong dữ liệu, 
                    với 1 số lỗi còn lại tùy thuộc vào độ chính xác của mô hình (Data = model + Error)
                Step7: Data product
                    * Sản phẩm dữ liệu là một ứng dụng máy tính nhận dữ liệu đầu vào và đầu ra, đưa chúng trở lại môi trường
                    * Nó dự trên mô hình hoặc thuật toán
                    * Một ví dụ là một ứng dụng phân tích dữ liệu về lịch sử mua hàng của khách hàng và 
                    khuyến nghị các giao dịch mua khác mà khách hàng có thể được hưởng
                Step8: Communication
                    * Sau khi dữ liệu được phân tích, nó có thể được báo cáo theo nhiều định dạng cho người dùng để hỗ trọ các yêu cầu của họ.
                    Người dùng có thể có phản hồi, dẫn đến việc phân tích bổ sung
                    * Phần lớn chu trình phân tích lặp đi lặp lại
                Chốt vấn đề
                    * Khi xác định cách truyền đạt kết quả, nhà phân tích có thể xem xét các kỹ thuật trực quan hóa dữ liệu để giúp truyền đạt thông điệp rõ ràng và 
                    hiệu quả đến khán giả
                    * Trực quan hóa dữ liệu sử dụng hiển thị thông tin (Như bảng và biểu đồ) để giúp truyền đạt các thông điệp chính cho trong dữ liệu
                    - Các bảng hữu ích cho người dùng có thể tra cứu các số cụ thể, trong các biểu đồ (barplot, line plot...) 
                    có thể giúp giải thích thông điệp định lượng có trong dữ liệu
    B. Exploratory Data Analysis (EDA) - Phân tích khám phá dữ liệu
        1. Giới thiếu
            - EDA: là 1 cách tiếp cận để phân tích dữ liệu => EDA nghiên cứu 1 cái nhìn về dữ liệu và cố gắng hiểu về dữ liệu
            - EDA: quá trình quan trọng trong việc thực hiện điều tra ban đầu về dữ liệu để khám phá các mẫu, phát hiện dị thường, kiểm tra giả thuyết và
            kiểm tra các giả định với sự trợ giúp của thống kê tóm tắt và biểu diễn đồ họa
            - EDA: liên quan đến việc nhà phân tích cố gắng để có được một cảm giác của người dùng cho bộ dữ liệu, thường sử dụng phán đoán của chính họ để
            xác định yếu tố quan trọng nhất trong bộ dữ liệu là gì
        2. Mục đích của EDA
            - Kiểm tra dữ liệu bị thiếu và các lỗi khác
            - Có được cái nhìn sâu sắc tối đa về tập dữ liệu và cấu trúc cơ bản của nó
            - Khám phá 1 mô hình tốt, một mô hình giải thích dữ liệu với số lượng biến dự đoán tối thiểu
            - Kiểm tra các giả định liên quan đến bất kỳ mô hình phù hợp hoặc kiểm tra giả định
            - Tạo ra các danh sách ngoại lệ hoặc dị thường khác
            - Tìm ước tính tham số và khoảng tin cậy liên quan hoặc sai số
            - Xác định các biến có ảnh hưởng nhất
            - Kiến thức cụ thể khác có thể có được thông qua EDA như tạo danh sách xếp hạng các yếu tố liên quan. Có thể không nhất thiết phải có tất cả các mục trên
            trong phân tích dữ liệu
            

'''